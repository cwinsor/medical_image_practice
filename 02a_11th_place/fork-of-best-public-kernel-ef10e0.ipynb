{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5076bb76",
   "metadata": {
    "papermill": {
     "duration": 0.023701,
     "end_time": "2022-07-12T03:38:59.628201",
     "exception": false,
     "start_time": "2022-07-12T03:38:59.604500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🛠 Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bec0780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:38:59.679105Z",
     "iopub.status.busy": "2022-07-12T03:38:59.677347Z",
     "iopub.status.idle": "2022-07-12T03:38:59.679701Z",
     "shell.execute_reply": "2022-07-12T03:38:59.680143Z",
     "shell.execute_reply.started": "2022-07-10T16:29:22.449683Z"
    },
    "papermill": {
     "duration": 0.030236,
     "end_time": "2022-07-12T03:38:59.680403",
     "exception": false,
     "start_time": "2022-07-12T03:38:59.650167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q ../input/mmlablibsv2/einops-0.4.1-py3-none-any.whl\n",
    "# !pip install -q ../input/mmlablibsv2/yapf-0.31.0-py2.py3-none-any.whl\n",
    "# !pip install -q ../input/mmlablibsv2/addict-2.4.0-py3-none-any.whl\n",
    "# !pip install -q ../input/mmlablibsv2/terminaltables-3.1.0-py3-none-any.whl\n",
    "# !pip install -q ../input/mmlablibsv2/mmcv_full-1.3.17-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8854d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:38:59.748949Z",
     "iopub.status.busy": "2022-07-12T03:38:59.725876Z",
     "iopub.status.idle": "2022-07-12T03:43:08.015861Z",
     "shell.execute_reply": "2022-07-12T03:43:08.015287Z",
     "shell.execute_reply.started": "2022-07-10T16:29:22.602074Z"
    },
    "papermill": {
     "duration": 248.314613,
     "end_time": "2022-07-12T03:43:08.016024",
     "exception": false,
     "start_time": "2022-07-12T03:38:59.701411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\r\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\r\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Skipping tokenizers as it is not installed.\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/uwmgiseg/dependencies/huggingface_hub-0.5.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.5.1) (5.4.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.5.1) (21.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.5.1) (2.25.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.5.1) (4.8.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.5.1) (3.0.12)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.5.1) (4.62.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.5.1) (3.10.0.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub==0.5.1) (2.4.7)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub==0.5.1) (3.5.0)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub==0.5.1) (4.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub==0.5.1) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub==0.5.1) (1.26.6)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub==0.5.1) (2.10)\r\n",
      "Installing collected packages: huggingface-hub\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.0.19\r\n",
      "    Uninstalling huggingface-hub-0.0.19:\r\n",
      "      Successfully uninstalled huggingface-hub-0.0.19\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "allennlp 2.7.0 requires transformers<4.10,>=4.1, which is not installed.\r\n",
      "datasets 1.12.1 requires huggingface-hub<0.1.0,>=0.0.14, but you have huggingface-hub 0.5.1 which is incompatible.\u001b[0m\r\n",
      "Successfully installed huggingface-hub-0.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "allennlp 2.7.0 requires transformers<4.10,>=4.1, but you have transformers 4.18.0 which is incompatible.\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "running develop\r\n",
      "running egg_info\r\n",
      "creating cvcore.egg-info\r\n",
      "writing cvcore.egg-info/PKG-INFO\r\n",
      "writing dependency_links to cvcore.egg-info/dependency_links.txt\r\n",
      "writing requirements to cvcore.egg-info/requires.txt\r\n",
      "writing top-level names to cvcore.egg-info/top_level.txt\r\n",
      "writing manifest file 'cvcore.egg-info/SOURCES.txt'\r\n",
      "reading manifest file 'cvcore.egg-info/SOURCES.txt'\r\n",
      "writing manifest file 'cvcore.egg-info/SOURCES.txt'\r\n",
      "running build_ext\r\n",
      "Creating /opt/conda/lib/python3.7/site-packages/cvcore.egg-link (link to .)\r\n",
      "Adding cvcore 0.0.1 to easy-install.pth file\r\n",
      "\r\n",
      "Installed /kaggle/working\r\n",
      "Processing dependencies for cvcore==0.0.1\r\n",
      "Searching for torchvision==0.10.1\r\n",
      "Best match: torchvision 0.10.1\r\n",
      "Adding torchvision 0.10.1 to easy-install.pth file\r\n",
      "\r\n",
      "Using /opt/conda/lib/python3.7/site-packages\r\n",
      "Searching for torch==1.9.1\r\n",
      "Best match: torch 1.9.1\r\n",
      "Adding torch 1.9.1 to easy-install.pth file\r\n",
      "Installing convert-caffe2-to-onnx script to /opt/conda/bin\r\n",
      "Installing convert-onnx-to-caffe2 script to /opt/conda/bin\r\n",
      "\r\n",
      "Using /opt/conda/lib/python3.7/site-packages\r\n",
      "Searching for numpy==1.19.5\r\n",
      "Best match: numpy 1.19.5\r\n",
      "Adding numpy 1.19.5 to easy-install.pth file\r\n",
      "Installing f2py script to /opt/conda/bin\r\n",
      "Installing f2py3 script to /opt/conda/bin\r\n",
      "Installing f2py3.7 script to /opt/conda/bin\r\n",
      "\r\n",
      "Using /opt/conda/lib/python3.7/site-packages\r\n",
      "Searching for Pillow==8.2.0\r\n",
      "Best match: Pillow 8.2.0\r\n",
      "Adding Pillow 8.2.0 to easy-install.pth file\r\n",
      "\r\n",
      "Using /opt/conda/lib/python3.7/site-packages\r\n",
      "Searching for typing-extensions==3.10.0.2\r\n",
      "Best match: typing-extensions 3.10.0.2\r\n",
      "Adding typing-extensions 3.10.0.2 to easy-install.pth file\r\n",
      "\r\n",
      "Using /opt/conda/lib/python3.7/site-packages\r\n",
      "Finished processing dependencies for cvcore==0.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qy transformers\n",
    "!pip uninstall -qy tokenizers\n",
    "!pip install -q ../input/pytorch-segmentation-models-lib/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4\n",
    "!pip install -q ../input/pytorch-segmentation-models-lib/efficientnet_pytorch-0.6.3/efficientnet_pytorch-0.6.3\n",
    "!pip install -q ../input/segmentation-models-pytorch-030/timm-0.5.4-py3-none-any.whl\n",
    "!pip install -q ../input/segmentation-models-pytorch-030/segmentation_models_pytorch-0.3.0.dev0-py3-none-any.whl\n",
    "!pip uninstall -qy transformers\n",
    "!pip uninstall -qy tokenizers\n",
    "!pip install -q ../input/uwmgiseg/dependencies/tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
    "!pip install ../input/uwmgiseg/dependencies/huggingface_hub-0.5.1-py3-none-any.whl\n",
    "!pip install -q ../input/uwmgiseg/dependencies/transformers-4.18.0-py3-none-any.whl\n",
    "!pip install -q ../input/uwmgiseg/dependencies/monai-0.8.1-202202162213-py3-none-any.whl\n",
    "!python ../input/uwmgiseg/setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb359fbe",
   "metadata": {
    "papermill": {
     "duration": 0.027392,
     "end_time": "2022-07-12T03:43:08.071095",
     "exception": false,
     "start_time": "2022-07-12T03:43:08.043703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📚 Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8db15e9f",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:08.139161Z",
     "iopub.status.busy": "2022-07-12T03:43:08.138187Z",
     "iopub.status.idle": "2022-07-12T03:43:20.467756Z",
     "shell.execute_reply": "2022-07-12T03:43:20.467190Z",
     "shell.execute_reply.started": "2022-07-10T16:33:20.873141Z"
    },
    "papermill": {
     "duration": 12.368597,
     "end_time": "2022-07-12T03:43:20.467946",
     "exception": false,
     "start_time": "2022-07-12T03:43:08.099349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 2 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import random\n",
    "from glob import glob\n",
    "import os, shutil\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import copy\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from IPython import display as ipd\n",
    "\n",
    "# visualization\n",
    "import cv2\n",
    "import cupy as cp\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "c_  = Fore.GREEN\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "from segmentation_models_pytorch.base.modules import Activation\n",
    "from segmentation_models_pytorch.base import modules as md\n",
    "from segmentation_models_pytorch.decoders.deeplabv3.decoder import ASPP, SeparableConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109a10ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:20.531210Z",
     "iopub.status.busy": "2022-07-12T03:43:20.530542Z",
     "iopub.status.idle": "2022-07-12T03:43:21.789975Z",
     "shell.execute_reply": "2022-07-12T03:43:21.789041Z",
     "shell.execute_reply.started": "2022-07-10T16:33:32.067444Z"
    },
    "papermill": {
     "duration": 1.293699,
     "end_time": "2022-07-12T03:43:21.790126",
     "exception": false,
     "start_time": "2022-07-12T03:43:20.496427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../input/uwmgiseg\")\n",
    "import cvcore\n",
    "from cvcore.config import get_cfg\n",
    "from cvcore.modeling.meta_arch import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5af5c6",
   "metadata": {
    "papermill": {
     "duration": 0.028045,
     "end_time": "2022-07-12T03:43:21.846891",
     "exception": false,
     "start_time": "2022-07-12T03:43:21.818846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🔨 Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f098aaf1",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:21.930851Z",
     "iopub.status.busy": "2022-07-12T03:43:21.929721Z",
     "iopub.status.idle": "2022-07-12T03:43:21.932349Z",
     "shell.execute_reply": "2022-07-12T03:43:21.931883Z",
     "shell.execute_reply.started": "2022-07-10T16:33:33.196781Z"
    },
    "papermill": {
     "duration": 0.056391,
     "end_time": "2022-07-12T03:43:21.932484",
     "exception": false,
     "start_time": "2022-07-12T03:43:21.876093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask2rle(msk):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    msk    = cp.array(msk)\n",
    "    pixels = msk.flatten()\n",
    "    pad    = cp.array([0])\n",
    "    pixels = cp.concatenate([pad, pixels, pad])\n",
    "    runs   = cp.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "def read_image(name):\n",
    "    img = cv2.imread(name, cv2.IMREAD_ANYDEPTH) / 65535.0\n",
    "    img = 255 * ((img - img.min()) / (img.max() - img.min()))\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def df_preprocessing(df, globbed_file_list):\n",
    "    \"\"\"The preprocessing steps applied to get column information\"\"\"\n",
    "    # 1. Get Case-ID as a column (str and int)\n",
    "    df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n",
    "    df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n",
    "\n",
    "    # 2. Get Day as a column\n",
    "    df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n",
    "    df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n",
    "\n",
    "    # 3. Get Slice Identifier as a column\n",
    "    df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n",
    "\n",
    "    # 4. Get full file paths for the representative scans\n",
    "    df[\"_partial_ident\"] = (\n",
    "        globbed_file_list[0].rsplit(\"/\", 4)[0]\n",
    "        + \"/\"\n",
    "        + df[\"case_id_str\"]  # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n",
    "        + \"/\"\n",
    "        + df[\"case_id_str\"]  # .../case###/\n",
    "        + \"_\"\n",
    "        + df[\"day_num_str\"]\n",
    "        + \"/scans/\"  # .../case###_day##/\n",
    "        + df[\"slice_id\"]\n",
    "    )  # .../slice_####\n",
    "    _tmp_merge_df = pd.DataFrame(\n",
    "        {\n",
    "            \"_partial_ident\": [x.rsplit(\"_\", 4)[0] for x in globbed_file_list],\n",
    "            \"f_path\": globbed_file_list,\n",
    "        }\n",
    "    )\n",
    "    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n",
    "\n",
    "    # 5. Get slice dimensions from filepath (int in pixels)\n",
    "    df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\", 4)[1]))\n",
    "    df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\", 4)[2]))\n",
    "\n",
    "    # 6. Pixel spacing from filepath (float in mm)\n",
    "    df[\"px_spacing_h\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\", 4)[3]))\n",
    "    df[\"px_spacing_w\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\", 4)[4]))\n",
    "\n",
    "    # 7. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n",
    "    new_col_order = [\n",
    "        \"id\",\n",
    "        \"f_path\",\n",
    "        \"slice_h\",\n",
    "        \"slice_w\",\n",
    "        \"px_spacing_h\",\n",
    "        \"px_spacing_w\",\n",
    "        \"case_id_str\",\n",
    "        \"case_id\",\n",
    "        \"day_num_str\",\n",
    "        \"day_num\",\n",
    "        \"slice_id\",\n",
    "    ]\n",
    "    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n",
    "    df = df[new_col_order]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_nearby_slices(id_, case_id_str, day_num_str, case_length, num_slices=3, num_strides=1):\n",
    "    slice_idx = int(id_.split(\"_\")[-1])\n",
    "    get_idxs = np.arange(slice_idx - num_slices//2 * num_strides, \n",
    "                         slice_idx + num_slices//2 * num_strides + 1, num_strides) # -7 -5 -3 -1 1 3 5 7 9\n",
    "    \n",
    "    min_idx = 2 if slice_idx%2 == 0 else 1\n",
    "    if case_length % 2 == 0:\n",
    "        max_idx = case_length if slice_idx%2 == 0 else case_length - 1\n",
    "    else:\n",
    "        max_idx = case_length if slice_idx%2 != 0 else case_length - 1\n",
    "\n",
    "    get_idxs = np.clip(get_idxs, min_idx, max_idx)\n",
    "    get_ids = [f\"{case_id_str}_{day_num_str}_slice_{slice_idx:04d}\" for slice_idx in get_idxs]\n",
    "    return get_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d0ffb1",
   "metadata": {
    "papermill": {
     "duration": 0.026829,
     "end_time": "2022-07-12T03:43:21.987602",
     "exception": false,
     "start_time": "2022-07-12T03:43:21.960773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c7bf55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:22.048342Z",
     "iopub.status.busy": "2022-07-12T03:43:22.047557Z",
     "iopub.status.idle": "2022-07-12T03:43:22.051086Z",
     "shell.execute_reply": "2022-07-12T03:43:22.050625Z",
     "shell.execute_reply.started": "2022-07-10T16:33:33.22033Z"
    },
    "papermill": {
     "duration": 0.036122,
     "end_time": "2022-07-12T03:43:22.051215",
     "exception": false,
     "start_time": "2022-07-12T03:43:22.015093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/uw-madison-gi-tract-image-segmentation/\"\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "SUB_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dddceb77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:22.115504Z",
     "iopub.status.busy": "2022-07-12T03:43:22.114855Z",
     "iopub.status.idle": "2022-07-12T03:43:29.211360Z",
     "shell.execute_reply": "2022-07-12T03:43:29.210765Z",
     "shell.execute_reply.started": "2022-07-10T16:33:33.240295Z"
    },
    "papermill": {
     "duration": 7.132236,
     "end_time": "2022-07-12T03:43:29.211512",
     "exception": false,
     "start_time": "2022-07-12T03:43:22.079276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(SUB_CSV)\n",
    "\n",
    "if not len(sub_df):\n",
    "    # Infer on train cases\n",
    "    debug = True \n",
    "    sub_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "    sub_df = sub_df.drop(columns=['class','segmentation']).drop_duplicates()\n",
    "    paths = glob(f'/kaggle/input/uw-madison-gi-tract-image-segmentation/train/**/*png', recursive=True)\n",
    "    sub_df = df_preprocessing(sub_df, paths)\n",
    "    cases = sub_df[\"case_id_str\"].unique()[:1]\n",
    "    sub_df = sub_df[sub_df[\"case_id_str\"].isin(cases)].reset_index(drop=True)\n",
    "else:\n",
    "    debug = False\n",
    "    sub_df = sub_df.drop(columns=['class','predicted']).drop_duplicates()\n",
    "    paths = glob(f'/kaggle/input/uw-madison-gi-tract-image-segmentation/test/**/*png',recursive=True)\n",
    "    sub_df = df_preprocessing(sub_df, paths)\n",
    "    sub_df = sub_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e63f7",
   "metadata": {
    "papermill": {
     "duration": 0.028855,
     "end_time": "2022-07-12T03:43:29.268244",
     "exception": false,
     "start_time": "2022-07-12T03:43:29.239389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ebfe45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:29.332932Z",
     "iopub.status.busy": "2022-07-12T03:43:29.331864Z",
     "iopub.status.idle": "2022-07-12T03:43:29.333844Z",
     "shell.execute_reply": "2022-07-12T03:43:29.334380Z",
     "shell.execute_reply.started": "2022-07-10T16:33:38.912497Z"
    },
    "papermill": {
     "duration": 0.038555,
     "end_time": "2022-07-12T03:43:29.334533",
     "exception": false,
     "start_time": "2022-07-12T03:43:29.295978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sys.path.append(\"../input/uwsegv2\")\n",
    "# from segmentation_models_pytorch.base import ClassificationHead as TClassificationHead\n",
    "# from giseg.cvcore.modeling.backbone import TimmUniversalEncoder as TTimmUniversalEncoder\n",
    "# from giseg.cvcore.modeling.backbone import mit_PLD_b4\n",
    "# from giseg.cvcore.modeling.heads import UnetDecoder as TUnetDecoder\n",
    "# from giseg.cvcore.modeling.heads import ASPPHead\n",
    "# from giseg.cvcore.modeling.heads import SegmentationHead as TSegmentationHead\n",
    "\n",
    "# class TimmUNetASPP(nn.Module):\n",
    "#     def __init__(self, arch, img_size, num_slices, num_strides):\n",
    "#         super(TimmUNetASPP, self).__init__()\n",
    "#         num_slices = 31\n",
    "#         self.encoder = TTimmUniversalEncoder(\n",
    "#             arch,\n",
    "#             pretrained=False,\n",
    "#             in_channels=num_slices,\n",
    "#             drop_path_rate=None,\n",
    "#             img_size=img_size[0],\n",
    "#         )\n",
    "#         encoder_channels = list(self.encoder.out_channels)\n",
    "#         if len(encoder_channels) == 5 or  len(encoder_channels) == 6: \n",
    "#             common_stride = 2\n",
    "#         else:\n",
    "#             common_stride = 1\n",
    "#         decoder_channels = [2048, 1024, 512, 256]\n",
    "#         n_blocks = len(decoder_channels)\n",
    "#         num_classes = 3\n",
    "\n",
    "#         self.decoder = TUnetDecoder(\n",
    "#             encoder_channels,\n",
    "#             decoder_channels,\n",
    "#             n_blocks=n_blocks,\n",
    "#             center=False,\n",
    "#             attention_type='scse',\n",
    "#             norm=\"BN\",\n",
    "#             act=\"relu\",\n",
    "#         )\n",
    "#         self.segmentation_head = TSegmentationHead(\n",
    "#             in_channels=decoder_channels[-1],\n",
    "#             out_channels=num_classes,\n",
    "#             upsampling=common_stride,\n",
    "#         )\n",
    "#         self.aux_decoder = ASPPHead(\n",
    "#                             encoder_channels = encoder_channels,\n",
    "#         )\n",
    "        \n",
    "#         self.segmentation_head_aux = TSegmentationHead(\n",
    "#             in_channels=decoder_channels[-1],\n",
    "#             out_channels=num_classes,\n",
    "#             upsampling=4,\n",
    "#         )\n",
    "#         self.classification_head = TClassificationHead(\n",
    "#             in_channels=self.encoder.out_channels[-1], classes=num_classes\n",
    "#         )\n",
    "#         self._add_hausdorff = False\n",
    "\n",
    "#     @autocast()\n",
    "#     def forward(self, images, gt_masks=None, image_sizes=None):\n",
    "#         features = self.encoder(images)\n",
    "#         decoder_output = self.decoder(*features)\n",
    "#         masks = self.segmentation_head(decoder_output)\n",
    "#         ## No need to infer this auxilary branch \n",
    "# #         aux_out = self.aux_decoder(*features) \n",
    "# #         aux_mask = self.segmentation_head_aux(aux_out)\n",
    "#         if self.training:\n",
    "#             losses = seg_criterion(masks, gt_masks, self._add_hausdorff)\n",
    "#             losses.update(seg_criterion(aux_mask,gt_masks, self._add_hausdorff, weights=[1.,1.], aux=True))\n",
    "#             gt_classes = (gt_masks.sum((2, 3)) > 0).float()\n",
    "#             cls_logits = self.classification_head(features[-1])\n",
    "#             cls_loss = cls_criterion(cls_logits, gt_classes)\n",
    "#             losses.update({\"bce_cls\": cls_loss})\n",
    "#             return losses\n",
    "#         else:\n",
    "#             # masks = masks.view(masks.shape[0], -1, 3, masks.shape[2], masks.shape[3])\n",
    "#             # masks = masks[:, masks.shape[1] // 2, ...]\n",
    "# #             return (masks + aux_mask)/2\n",
    "#             return torch.sigmoid(masks)\n",
    "\n",
    "# class TimmssFormerASPP(nn.Module):\n",
    "#     def __init__(self, img_size, num_slices, num_strides):\n",
    "#         super(TimmssFormerASPP, self).__init__()\n",
    "\n",
    "#         if num_strides == 1:\n",
    "#             num_slices = num_slices // num_strides + 1\n",
    "#         else:\n",
    "#             num_slices = num_slices\n",
    "#         num_classes = 3\n",
    "\n",
    "#         self.encoderdecoder = mit_PLD_b4(class_num=num_classes)\n",
    "#         self.segmentation_head = TSegmentationHead(\n",
    "#             in_channels=128,\n",
    "#             out_channels=num_classes,\n",
    "#             upsampling=4, \n",
    "#         )\n",
    "#         self.classification_head = TClassificationHead(\n",
    "#             in_channels=512, classes=num_classes\n",
    "#         )\n",
    "#         self._add_hausdorff = False\n",
    "\n",
    "#     @autocast()\n",
    "#     def forward(self, images, gt_masks=None, image_sizes=None):\n",
    "#         features, masks = self.encoderdecoder(images)\n",
    "#         if self.training:\n",
    "#             losses = seg_criterion(masks, gt_masks, self._add_hausdorff)\n",
    "#             gt_classes = (gt_masks.sum((2, 3)) > 0).float()\n",
    "#             cls_logits = self.classification_head(features[-1])\n",
    "#             cls_loss = cls_criterion(cls_logits, gt_classes)\n",
    "#             losses.update({\"bce_cls\": cls_loss})\n",
    "#             return losses\n",
    "#         else:\n",
    "#             # masks = masks.view(masks.shape[0], -1, 3, masks.shape[2], masks.shape[3])\n",
    "#             # masks = masks[:, masks.shape[1] // 2, ...]\n",
    "#             # return masks \n",
    "#             return torch.sigmoid(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d1205d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:29.403522Z",
     "iopub.status.busy": "2022-07-12T03:43:29.402495Z",
     "iopub.status.idle": "2022-07-12T03:43:29.404488Z",
     "shell.execute_reply": "2022-07-12T03:43:29.404905Z",
     "shell.execute_reply.started": "2022-07-10T16:33:38.926455Z"
    },
    "papermill": {
     "duration": 0.04249,
     "end_time": "2022-07-12T03:43:29.405082",
     "exception": false,
     "start_time": "2022-07-12T03:43:29.362592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimmUniversalEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        pretrained=False,\n",
    "        in_channels=3,\n",
    "        drop_path_rate=0.0,\n",
    "        depth=5,\n",
    "        output_stride=32,\n",
    "        img_size=224,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        kwargs = dict(\n",
    "            in_chans=in_channels,\n",
    "            features_only=True,\n",
    "            pretrained=pretrained,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            img_size=img_size,\n",
    "        )\n",
    "        kwargs.pop(\"img_size\")\n",
    "        self.model = timm.create_model(name, **kwargs)\n",
    "        if name.startswith('convnext'):\n",
    "            old_conv = self.model.stages_3.downsample[1]\n",
    "            old_in, old_out = old_conv.in_channels, old_conv.out_channels\n",
    "            self.model.stages_3.downsample[1] = nn.Conv2d(\n",
    "                old_in, old_out, kernel_size=1, stride=1\n",
    "            )\n",
    "            self.model.stages_3.downsample[1].weight.data = old_conv.weight.mean(\n",
    "                dim=(2, 3), keepdim=True\n",
    "            )\n",
    "            self.model.stages_3.downsample[1].bias.data = old_conv.bias\n",
    "        self._out_channels = [\n",
    "            in_channels,\n",
    "        ] + self.model.feature_info.channels()\n",
    "        self._depth = depth\n",
    "        self._output_stride = output_stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        features = [\n",
    "            x,\n",
    "        ] + features\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self._out_channels\n",
    "\n",
    "    @property\n",
    "    def output_stride(self):\n",
    "        return min(self._output_stride, 2 ** self._depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f946b6f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:29.476191Z",
     "iopub.status.busy": "2022-07-12T03:43:29.475202Z",
     "iopub.status.idle": "2022-07-12T03:43:29.478157Z",
     "shell.execute_reply": "2022-07-12T03:43:29.477613Z",
     "shell.execute_reply.started": "2022-07-10T16:33:38.953974Z"
    },
    "papermill": {
     "duration": 0.044784,
     "end_time": "2022-07-12T03:43:29.478301",
     "exception": false,
     "start_time": "2022-07-12T03:43:29.433517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
    "        activation = Activation(activation)\n",
    "        super().__init__(conv2d, upsampling, activation)\n",
    "\n",
    "class SegmentationHeadDouble(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, activation_func=None, upsampling_scale=1):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling_scale) if upsampling_scale > 1 else nn.Identity()\n",
    "        activation = Activation(activation_func)\n",
    "        conv2d_2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling_2 = nn.UpsamplingBilinear2d(scale_factor=upsampling_scale) if upsampling_scale > 1 else nn.Identity()\n",
    "        activation_2 = Activation(activation_func)\n",
    "        super().__init__(conv2d, upsampling, activation, conv2d_2, upsampling_2, activation_2)\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None):\n",
    "        if pooling not in (\"max\", \"avg\"):\n",
    "            raise ValueError(\"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling))\n",
    "        pool = nn.AdaptiveAvgPool2d(1) if pooling == \"avg\" else nn.AdaptiveMaxPool2d(1)\n",
    "        flatten = nn.Flatten()\n",
    "        dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n",
    "        linear = nn.Linear(in_channels, classes, bias=True)\n",
    "        activation = Activation(activation)\n",
    "        super().__init__(pool, flatten, dropout, linear, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f9f3b2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:29.580591Z",
     "iopub.status.busy": "2022-07-12T03:43:29.559447Z",
     "iopub.status.idle": "2022-07-12T03:43:29.594271Z",
     "shell.execute_reply": "2022-07-12T03:43:29.593370Z",
     "shell.execute_reply.started": "2022-07-10T16:33:38.971419Z"
    },
    "papermill": {
     "duration": 0.088223,
     "end_time": "2022-07-12T03:43:29.594413",
     "exception": false,
     "start_time": "2022-07-12T03:43:29.506190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepLabV3PlusDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels,\n",
    "        out_channels=256,\n",
    "        atrous_rates=(12, 24, 36),\n",
    "        output_stride=16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if output_stride not in {8, 16}:\n",
    "            raise ValueError(\"Output stride should be 8 or 16, got {}.\".format(output_stride))\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.output_stride = output_stride\n",
    "\n",
    "        self.aspp = nn.Sequential(\n",
    "            ASPP(encoder_channels[-1], out_channels, atrous_rates, separable=True),\n",
    "            SeparableConv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        scale_factor = 2 if output_stride == 8 else 4\n",
    "        self.up = nn.UpsamplingBilinear2d(scale_factor=scale_factor)\n",
    "\n",
    "        highres_in_channels = encoder_channels[-4]\n",
    "        highres_out_channels = 48  # proposed by authors of paper\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(highres_in_channels, highres_out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(highres_out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            SeparableConv2d(\n",
    "                highres_out_channels + out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, *features):\n",
    "        aspp_features = self.aspp(features[-1])\n",
    "        aspp_features = self.up(aspp_features)\n",
    "        high_res_features = self.block1(features[-4])\n",
    "        concat_features = torch.cat([aspp_features, high_res_features], dim=1)\n",
    "        fused_features = self.block2(concat_features)\n",
    "        return fused_features\n",
    "\n",
    "    \n",
    "class DeepLabV3PlusDecoderFix(DeepLabV3PlusDecoder):\n",
    "    def __init__(self, encoder_channels):\n",
    "        super().__init__(encoder_channels,)\n",
    "        aspp_channels = 256\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(aspp_channels, aspp_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(aspp_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up_2 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "    def forward(self, *features):\n",
    "        aspp_features = self.aspp(features[-1])\n",
    "        aspp_features = self.up(aspp_features)\n",
    "        aspp_features = self.block3(aspp_features)\n",
    "        aspp_features = self.up_2(aspp_features)\n",
    "        high_res_features = self.block1(features[-4])\n",
    "        concat_features = torch.cat([aspp_features, high_res_features], dim=1)\n",
    "        fused_features = self.block2(concat_features)\n",
    "        return fused_features\n",
    "    \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels,\n",
    "        use_batchnorm=True,\n",
    "        attention_type=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
    "\n",
    "    def forward(self, x, skip=None, scale=True):\n",
    "        if scale:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CenterBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n",
    "        conv1 = md.Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        super().__init__(conv1, conv2)\n",
    "        \n",
    "        \n",
    "class UnetPlusPlusDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels,\n",
    "        decoder_channels=(256, 128, 64, 32, 16),\n",
    "        n_blocks=5,\n",
    "        use_batchnorm=True,\n",
    "        attention_type=None,\n",
    "        center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        self.in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        self.skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        self.out_channels = decoder_channels\n",
    "        if center:\n",
    "            self.center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n",
    "\n",
    "        blocks = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(layer_idx + 1):\n",
    "                if depth_idx == 0:\n",
    "                    in_ch = self.in_channels[layer_idx]\n",
    "                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1)\n",
    "                    out_ch = self.out_channels[layer_idx]\n",
    "                else:\n",
    "                    out_ch = self.skip_channels[layer_idx]\n",
    "                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1 - depth_idx)\n",
    "                    in_ch = self.skip_channels[layer_idx - 1]\n",
    "                blocks[f\"x_{depth_idx}_{layer_idx}\"] = DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n",
    "        blocks[f\"x_{0}_{len(self.in_channels)-1}\"] = DecoderBlock(\n",
    "            self.in_channels[-1], 0, self.out_channels[-1], **kwargs\n",
    "        )\n",
    "        self.blocks = nn.ModuleDict(blocks)\n",
    "        self.depth = len(self.in_channels) - 1\n",
    "\n",
    "    def forward(self, *features):\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "        # start building dense connections\n",
    "        dense_x = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(self.depth - layer_idx):\n",
    "                if layer_idx == 0:\n",
    "                    output = self.blocks[f\"x_{depth_idx}_{depth_idx}\"](features[depth_idx], features[depth_idx + 1])\n",
    "                    dense_x[f\"x_{depth_idx}_{depth_idx}\"] = output\n",
    "                else:\n",
    "                    dense_l_i = depth_idx + layer_idx\n",
    "                    cat_features = [dense_x[f\"x_{idx}_{dense_l_i}\"] for idx in range(depth_idx + 1, dense_l_i + 1)]\n",
    "                    cat_features = torch.cat(cat_features + [features[dense_l_i + 1]], dim=1)\n",
    "                    dense_x[f\"x_{depth_idx}_{dense_l_i}\"] = self.blocks[f\"x_{depth_idx}_{dense_l_i}\"](\n",
    "                        dense_x[f\"x_{depth_idx}_{dense_l_i-1}\"], cat_features\n",
    "                    )\n",
    "        dense_x[f\"x_{0}_{self.depth}\"] = self.blocks[f\"x_{0}_{self.depth}\"](dense_x[f\"x_{0}_{self.depth-1}\"])\n",
    "        return dense_x[f\"x_{0}_{self.depth}\"]\n",
    "\n",
    "\n",
    "class UnetPlusPlusDecoderFix(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels,\n",
    "        decoder_channels=(256, 128, 64, 32),\n",
    "        n_blocks=4,\n",
    "        use_batchnorm=True,\n",
    "        attention_type=None,\n",
    "        center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        self.in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        self.skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        self.out_channels = decoder_channels\n",
    "        if center:\n",
    "            self.center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n",
    "\n",
    "        blocks = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(layer_idx + 1):\n",
    "                if depth_idx == 0:\n",
    "                    in_ch = self.in_channels[layer_idx]\n",
    "                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1)\n",
    "                    out_ch = self.out_channels[layer_idx]\n",
    "                else:\n",
    "                    out_ch = self.skip_channels[layer_idx]\n",
    "                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1 - depth_idx)\n",
    "                    in_ch = self.skip_channels[layer_idx - 1]\n",
    "                blocks[f\"x_{depth_idx}_{layer_idx}\"] = DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n",
    "        blocks[f\"x_{0}_{len(self.in_channels)-1}\"] = DecoderBlock(\n",
    "            self.in_channels[-1], 0, self.out_channels[-1], **kwargs\n",
    "        )\n",
    "        self.blocks = nn.ModuleDict(blocks)\n",
    "        self.depth = len(self.in_channels) - 1\n",
    "\n",
    "    def forward(self, *features):\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "        # start building dense connections\n",
    "        dense_x = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(self.depth - layer_idx):\n",
    "                if layer_idx == 0:\n",
    "                    output = self.blocks[f\"x_{depth_idx}_{depth_idx}\"](features[depth_idx], features[depth_idx + 1], depth_idx >= 1)\n",
    "                    dense_x[f\"x_{depth_idx}_{depth_idx}\"] = output\n",
    "                else:\n",
    "                    dense_l_i = depth_idx + layer_idx\n",
    "                    cat_features = [dense_x[f\"x_{idx}_{dense_l_i}\"] for idx in range(depth_idx + 1, dense_l_i + 1)]\n",
    "                    cat_features = torch.cat(cat_features + [features[dense_l_i + 1]], dim=1)\n",
    "                    dense_x[f\"x_{depth_idx}_{dense_l_i}\"] = self.blocks[f\"x_{depth_idx}_{dense_l_i}\"](\n",
    "                        dense_x[f\"x_{depth_idx}_{dense_l_i-1}\"], cat_features\n",
    "                    )\n",
    "        dense_x[f\"x_{0}_{self.depth}\"] = self.blocks[f\"x_{0}_{self.depth}\"](dense_x[f\"x_{0}_{self.depth-1}\"])\n",
    "        return dense_x[f\"x_{0}_{self.depth}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "279f7ed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:29.668811Z",
     "iopub.status.busy": "2022-07-12T03:43:29.665697Z",
     "iopub.status.idle": "2022-07-12T03:43:29.670991Z",
     "shell.execute_reply": "2022-07-12T03:43:29.671418Z",
     "shell.execute_reply.started": "2022-07-10T16:33:39.027229Z"
    },
    "papermill": {
     "duration": 0.048384,
     "end_time": "2022-07-12T03:43:29.671580",
     "exception": false,
     "start_time": "2022-07-12T03:43:29.623196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaselineSegTimm(nn.Module):\n",
    "    def __init__(self, index, num_stride):\n",
    "        super(BaselineSegTimm, self).__init__()\n",
    "\n",
    "        self.encoder = TimmUniversalEncoder(\n",
    "            CFG.ENCODER[index],\n",
    "            in_channels=num_stride,\n",
    "            img_size=CFG.img_size[index][0],\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_inputs = torch.randn(2, num_stride, *CFG.img_size[index])\n",
    "            out = self.encoder(dummy_inputs)\n",
    "            common_stride = CFG.img_size[index][0] // out[1].shape[2]\n",
    "        encoder_channels = self.encoder.out_channels\n",
    "        num_classes = 3\n",
    "\n",
    "        if CFG.ARCH[index] == \"DeepLabV3Plus\":\n",
    "            self.decoder = DeepLabV3PlusDecoder(\n",
    "                encoder_channels=encoder_channels,\n",
    "            )\n",
    "        elif CFG.ARCH[index] == \"DeepLabV3PlusFix\":\n",
    "            self.decoder = DeepLabV3PlusDecoderFix(\n",
    "                encoder_channels=encoder_channels,\n",
    "            )\n",
    "        elif CFG.ARCH[index] == \"UnetPlusPlus\":\n",
    "            self.decoder = UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels,\n",
    "            )\n",
    "        elif CFG.ARCH[index] == \"UnetPlusPlusFix\":\n",
    "            self.decoder = UnetPlusPlusDecoderFix(\n",
    "                encoder_channels=encoder_channels,\n",
    "            )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.decoder(*out)\n",
    "        \n",
    "        if CFG.ARCH[index] == \"UnetPlusPlus\":\n",
    "            self.segmentation_head = SegmentationHead(\n",
    "                    in_channels=out.shape[1],\n",
    "                    out_channels=num_classes,\n",
    "                    activation=None,\n",
    "                    kernel_size=3,\n",
    "                    upsampling=1,\n",
    "                )\n",
    "        elif CFG.ARCH[index] == \"UnetPlusPlusFix\":\n",
    "            self.segmentation_head = SegmentationHead(\n",
    "                    in_channels=out.shape[1],\n",
    "                    out_channels=num_classes,\n",
    "                    activation=None,\n",
    "                    kernel_size=3,\n",
    "                    upsampling=common_stride//2,\n",
    "                )\n",
    "        else:\n",
    "            if CFG.ENCODER[index].startswith('tf_efficientnet') or CFG.ENCODER[index].startswith('ecaresnet'):\n",
    "                self.segmentation_head = SegmentationHeadDouble(\n",
    "                    in_channels=out.shape[1],\n",
    "                    out_channels=num_classes,\n",
    "                    activation_func=None,\n",
    "                    kernel_size=3,\n",
    "                    upsampling_scale=common_stride,\n",
    "                )\n",
    "            else:\n",
    "                self.segmentation_head = SegmentationHead(\n",
    "                    in_channels=out.shape[1],\n",
    "                    out_channels=num_classes,\n",
    "                    activation=None,\n",
    "                    kernel_size=3,\n",
    "                    upsampling=common_stride,\n",
    "                )\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, images, gt_masks=None):\n",
    "        features = self.encoder(images)\n",
    "        decoder_output = self.decoder(*features)\n",
    "        masks = self.segmentation_head(decoder_output)\n",
    "        return torch.sigmoid(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "823a592d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:29.737348Z",
     "iopub.status.busy": "2022-07-12T03:43:29.736319Z",
     "iopub.status.idle": "2022-07-12T03:43:29.738556Z",
     "shell.execute_reply": "2022-07-12T03:43:29.739007Z",
     "shell.execute_reply.started": "2022-07-10T16:51:12.294599Z"
    },
    "papermill": {
     "duration": 0.039748,
     "end_time": "2022-07-12T03:43:29.739192",
     "exception": false,
     "start_time": "2022-07-12T03:43:29.699444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    ARCH = ['DeepLabV3Plus', 'DeepLabV3PlusFix', \n",
    "#             'DeepLabV3PlusFix',\n",
    "            'Unet',\n",
    "            'UnetPlusPlusFix', 'UnetPlusPlus',\n",
    "            'Unet',\n",
    "            'UnetPlusPlus'\n",
    "    ]\n",
    "    ENCODER = ['convnext_xlarge_in22ft1k', 'tf_efficientnetv2_l', \n",
    "#                'ecaresnet269d',\n",
    "               'convnext_base',\n",
    "               'convnext_xlarge_in22ft1k', 'tf_efficientnetv2_l',\n",
    "               'convnext_base',\n",
    "               'ecaresnet269d'\n",
    "    ]\n",
    "    WEIGHTS = [\n",
    "       '../input/newgiseg/deeplabv3plus_convnext_xlarge_17_608_fold-1_e28.pth',\n",
    "       '../input/uwmgiweights/deeplabv3plus_v2l_17_608_fold-1_e28.pth', \n",
    "#        '../input/uwmgiweights/deeplabv3plus_ecaresnet269d_17_608_long_fold-1_e98.pth',\n",
    "       '../input/uwmgiseg/weights/convnext_base_acs_unet_epoch30.pth',\n",
    "       '../input/uwmgiweights/unetplus_convnext_xlarge_fold-1_e28.pth',\n",
    "       '../input/uwmgiweights/unetplus_v2l_fold-1_e28.pth',\n",
    "       '../input/uwmgiseg/weights/convnext_base_unet_epoch10.pth',\n",
    "       '../input/uwmgiweights/unetplus_ecaresnet269d_fold-1_e98.pth'\n",
    "              ]\n",
    "    NUM_CLASSES = 3\n",
    "    \n",
    "\n",
    "    img_size = [[640, 640], [640, 640], \n",
    "#                 [640, 640], \n",
    "                [512, 512], [640, 640], [640, 640], [512, 512],\n",
    "                [640, 640]]\n",
    "    slices = [17, 17, \n",
    "#               17, \n",
    "              33, 9, 9, 5, 9]\n",
    "    strides = [2, 2, \n",
    "#                3, \n",
    "               1, 2, 3, 1, 2]\n",
    "#     THRESHOLDS = [0.3, 0.3, 0.4]\n",
    "    THRESHOLDS = [0.3, 0.3, 0.375]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c84fe98b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:29.815254Z",
     "iopub.status.busy": "2022-07-12T03:43:29.814171Z",
     "iopub.status.idle": "2022-07-12T03:43:29.816278Z",
     "shell.execute_reply": "2022-07-12T03:43:29.816673Z",
     "shell.execute_reply.started": "2022-07-10T16:40:12.47509Z"
    },
    "papermill": {
     "duration": 0.048657,
     "end_time": "2022-07-12T03:43:29.816873",
     "exception": false,
     "start_time": "2022-07-12T03:43:29.768216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GISegDataset(Dataset):\n",
    "    def __init__(self, df, target_size):\n",
    "        super().__init__()\n",
    "        resized_h, resized_w = target_size\n",
    "        df[\"case_day_str\"] = df[\"case_id_str\"] + \"_\" + df[\"day_num_str\"]\n",
    "        self.cases_length = df[\"case_day_str\"].value_counts().to_dict()\n",
    "        self.images_dict = {id: f_path for id, f_path in zip(df[\"id\"].values, df[\"f_path\"].values)}\n",
    "        self.aug512 = A.Compose([A.Resize(512, 512), ToTensorV2()])\n",
    "        self.aug608 = A.Compose([A.Resize(608, 608), ToTensorV2()])\n",
    "        self.aug640 = A.Compose([A.Resize(640, 640), ToTensorV2()])\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        info = self.df.iloc[idx]\n",
    "        center_id = info[\"id\"]\n",
    "        case_length = self.cases_length.get(info[\"case_day_str\"])\n",
    "        case_id = info[\"case_id_str\"]\n",
    "        day = info[\"day_num_str\"]\n",
    "        \n",
    "        ids172 = get_nearby_slices(center_id, case_id, day, case_length,\n",
    "                                   num_slices=17, num_strides=2)\n",
    "#         ids173 = get_nearby_slices(center_id, case_id, day, case_length,\n",
    "#                                    num_slices=17, num_strides=3)\n",
    "        ids331 = get_nearby_slices(center_id, case_id, day, case_length,\n",
    "                                   num_slices=33, num_strides=1)\n",
    "        ids92 = get_nearby_slices(center_id, case_id, day, case_length,\n",
    "                                  num_slices=9, num_strides=2)\n",
    "        ids93 = get_nearby_slices(center_id, case_id, day, case_length,\n",
    "                                  num_slices=9, num_strides=3)        \n",
    "#         img_ids = set(ids172 + ids173 + ids331 + ids92 + ids93)\n",
    "        img_ids = set(ids172 + ids331 + ids92 + ids93)\n",
    "        \n",
    "        imgs_dict = {id_: read_image(self.images_dict.get(id_)) for id_ in img_ids}\n",
    "        h, w = imgs_dict.get(ids172[0]).shape\n",
    "        \n",
    "        img172 = np.stack([imgs_dict.get(id_) for id_ in ids172], axis=-1)\n",
    "        img172 = self.aug640(image=img172)[\"image\"].float() / 255.\n",
    "        \n",
    "#         img173 = np.stack([imgs_dict.get(id_) for id_ in ids173], axis=-1)\n",
    "#         img173 = self.aug640(image=img173)[\"image\"].float() / 255.\n",
    "        \n",
    "        img331 = np.stack([imgs_dict.get(id_) for id_ in ids331], axis=-1)\n",
    "        img331 = self.aug512(image=img331)[\"image\"].float() / 255.\n",
    "        img51 = img331[14:19]\n",
    "        \n",
    "        idx92 = []; idx93 = []\n",
    "        for i in ids92:\n",
    "            idx92.append(ids172.index(i))\n",
    "        img92 = img172[idx92]\n",
    "#         for i in ids93:\n",
    "#             idx93.append(ids173.index(i))\n",
    "#         img93 = img173[idx93]\n",
    "\n",
    "        img93 = np.stack([imgs_dict.get(id_) for id_ in ids93], axis=-1)\n",
    "        img93 = self.aug640(image=img93)[\"image\"].float() / 255.\n",
    "        \n",
    "#         return img172, img173, img331, img92, img93, img51, center_id, h, w\n",
    "        return img172, img331, img92, img93, img51, center_id, h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad3a2d",
   "metadata": {
    "papermill": {
     "duration": 0.027196,
     "end_time": "2022-07-12T03:43:29.872310",
     "exception": false,
     "start_time": "2022-07-12T03:43:29.845114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🔭 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97346b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:43:29.943795Z",
     "iopub.status.busy": "2022-07-12T03:43:29.943155Z",
     "iopub.status.idle": "2022-07-12T03:47:18.516400Z",
     "shell.execute_reply": "2022-07-12T03:47:18.516979Z",
     "shell.execute_reply.started": "2022-07-10T16:33:39.083166Z"
    },
    "papermill": {
     "duration": 228.617205,
     "end_time": "2022-07-12T03:47:18.517167",
     "exception": false,
     "start_time": "2022-07-12T03:43:29.899962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/newgiseg/deeplabv3plus_convnext_xlarge_17_608_fold-1_e28.pth 0\n",
      "../input/uwmgiweights/deeplabv3plus_v2l_17_608_fold-1_e28.pth 0\n",
      "../input/uwmgiseg/weights/convnext_base_acs_unet_epoch30.pth 0.13078680634498596\n",
      "../input/uwmgiweights/unetplus_convnext_xlarge_fold-1_e28.pth 0\n",
      "../input/uwmgiweights/unetplus_v2l_fold-1_e28.pth 0\n",
      "../input/uwmgiseg/weights/convnext_base_unet_epoch10.pth 0.13846728205680847\n",
      "../input/uwmgiweights/unetplus_ecaresnet269d_fold-1_e98.pth 0\n"
     ]
    }
   ],
   "source": [
    "# all models\n",
    "def load_weight(wt, model, cls=False):\n",
    "    ckpt = torch.load(wt, \"cpu\")\n",
    "    if not cls:\n",
    "        ckpt[\"model\"] = {k: v for k, v in ckpt[\"model\"].items() if \"classification_head\" not in k}\n",
    "    else:\n",
    "        ckpt[\"model\"] = {k: v for k, v in ckpt[\"model\"].items()}\n",
    "    model.load_state_dict(ckpt.pop(\"model\"))\n",
    "    print(wt, ckpt[\"best_metric\"])\n",
    "    del ckpt; gc.collect()\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    return model\n",
    "\n",
    "all_models = []\n",
    "\n",
    "for i in range(len(CFG.WEIGHTS)):\n",
    "    if \"resnest200_unet_aspp\" in CFG.WEIGHTS[i]:\n",
    "        model = TimmUNetASPP(CFG.ENCODER[i],CFG.img_size[i],CFG.slices[i], CFG.strides[i])\n",
    "        model = load_weight(CFG.WEIGHTS[i], model, cls=True)\n",
    "    elif \"ssformer\" in CFG.WEIGHTS[i]:\n",
    "        model = TimmssFormerASPP(CFG.img_size[i],CFG.slices[i], CFG.strides[i])\n",
    "        model = load_weight(CFG.WEIGHTS[i], model, cls=True)\n",
    "    elif \"convnext_base_unet\" in CFG.WEIGHTS[i]:\n",
    "        cfg = get_cfg()\n",
    "        cfg.merge_from_file(f\"../input/uwmgiseg/configs/cb-unet.yaml\")\n",
    "        cfg.MODEL.BACKBONE.PRETRAINED = False\n",
    "        model = build_model(cfg)\n",
    "        model = load_weight(CFG.WEIGHTS[i], model)\n",
    "    elif \"convnext_base_acs_unet\" in CFG.WEIGHTS[i]:\n",
    "        cfg = get_cfg()\n",
    "        cfg.merge_from_file(f\"../input/uwmgiseg/configs/cb-acs-unet.yaml\")\n",
    "        cfg.MODEL.BACKBONE.PRETRAINED = False\n",
    "        model = build_model(cfg)\n",
    "        model = load_weight(CFG.WEIGHTS[i], model)\n",
    "    elif \"segformer_b5\" in  CFG.WEIGHTS[i]:\n",
    "        cfg.merge_from_file(f\"../input/uwmgiseg/configs/segb5.yaml\")\n",
    "        cfg.MODEL.BACKBONE.ARCH = \"../input/uwmgiseg/segformer-b5-finetuned-ade-640-640/\"\n",
    "        cfg.MODEL.BACKBONE.PRETRAINED = False\n",
    "        model = build_model(cfg)\n",
    "        model = load_weight(CFG.WEIGHTS[i], model)\n",
    "    else:\n",
    "        model = BaselineSegTimm(i, CFG.slices[i])\n",
    "        model = load_weight(CFG.WEIGHTS[i], model)\n",
    "    all_models.append(model)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b3396cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:47:18.597231Z",
     "iopub.status.busy": "2022-07-12T03:47:18.596220Z",
     "iopub.status.idle": "2022-07-12T03:47:18.598319Z",
     "shell.execute_reply": "2022-07-12T03:47:18.598764Z",
     "shell.execute_reply.started": "2022-07-10T16:51:15.243448Z"
    },
    "papermill": {
     "duration": 0.051026,
     "end_time": "2022-07-12T03:47:18.598946",
     "exception": false,
     "start_time": "2022-07-12T03:47:18.547920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masks2rles(masks, ids):\n",
    "    pred_strings = []; pred_ids = []; pred_classes = []\n",
    "    \n",
    "    for idx in range(len(masks)):\n",
    "        mask = masks[idx]\n",
    "        mask[0, :, :] = torch.where(mask[0, :, :] >= CFG.THRESHOLDS[0], 1, 0)\n",
    "        mask[1, :, :] = torch.where(mask[1, :, :] >= CFG.THRESHOLDS[1], 1, 0)\n",
    "        mask[2, :, :] = torch.where(mask[2, :, :] >= CFG.THRESHOLDS[2], 1, 0)\n",
    "        mask = mask.to(torch.uint8).permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "        rle = [None] * 3\n",
    "        for class_idx in [0, 1, 2]:\n",
    "            rle[class_idx] = mask2rle(mask[..., class_idx])\n",
    "\n",
    "        pred_strings.extend(rle)\n",
    "        pred_ids.extend([ids[idx]] * 3)\n",
    "        pred_classes.extend(['large_bowel', 'small_bowel', 'stomach'])\n",
    "\n",
    "    return pred_strings, pred_ids, pred_classes\n",
    "\n",
    "IMAGES = []\n",
    "MASKS = []\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(models, test_loader):\n",
    "    pred_strings = []; pred_ids = []; pred_classes = []\n",
    "    for imgs172, imgs331, imgs92, imgs93, imgs51, ids, heights, widths in tqdm(test_loader):\n",
    "        imgs172 = imgs172.half().cuda(non_blocking=True)\n",
    "        imgs331 = imgs331.half().cuda(non_blocking=True)\n",
    "        imgs92 = imgs92.half().cuda(non_blocking=True)\n",
    "        imgs93 = imgs93.half().cuda(non_blocking=True)\n",
    "        imgs51 = imgs51.half().cuda(non_blocking=True)\n",
    "        \n",
    "        masks = [0.0] * len(ids)\n",
    "        with autocast():\n",
    "            for model_index, model in enumerate(models):\n",
    "                if model_index <= 1:\n",
    "                    out = model(imgs172)\n",
    "                elif model_index == 2:\n",
    "                    out = model(imgs331)\n",
    "                elif model_index == 3:\n",
    "                    out = model(imgs92)\n",
    "                elif model_index == 4:\n",
    "                    out = model(imgs93)\n",
    "                elif model_index == 5:\n",
    "                    out = model(imgs51)\n",
    "                elif model_index == 6:\n",
    "                    out = model(imgs92)\n",
    "#                 out_h, out_w = out.size()[-2:]\n",
    "#                 assert [out_h, out_w] == CFG.img_size[model_index]\n",
    "                # Interpolation\n",
    "                for idx, (id_, mask, height, width) in enumerate(zip(ids, out, heights, widths)):\n",
    "                    mask = F.interpolate(mask.unsqueeze(0), size=(height.item(), width.item()), \n",
    "                                         mode='bilinear', align_corners=True)[0]\n",
    "                    masks[idx] += mask / len(models)\n",
    "        \n",
    "        result = masks2rles(masks, ids)\n",
    "        pred_strings.extend(result[0])\n",
    "        pred_ids.extend(result[1])\n",
    "        pred_classes.extend(result[2])\n",
    "        del result\n",
    "        gc.collect()\n",
    "    return pred_strings, pred_ids, pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "312a9a71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:47:18.676212Z",
     "iopub.status.busy": "2022-07-12T03:47:18.669893Z",
     "iopub.status.idle": "2022-07-12T03:56:01.268007Z",
     "shell.execute_reply": "2022-07-12T03:56:01.268449Z",
     "shell.execute_reply.started": "2022-07-10T16:51:16.398355Z"
    },
    "papermill": {
     "duration": 522.639761,
     "end_time": "2022-07-12T03:56:01.268629",
     "exception": false,
     "start_time": "2022-07-12T03:47:18.628868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [08:42<00:00,  4.84s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = GISegDataset(sub_df, CFG.img_size[0])\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, \n",
    "                         num_workers=2, shuffle=False, pin_memory=True)\n",
    "pred_strings, pred_ids, pred_classes = inference(\n",
    "    all_models, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fed5edd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:56:01.661274Z",
     "iopub.status.busy": "2022-07-12T03:56:01.660480Z",
     "iopub.status.idle": "2022-07-12T03:56:01.663921Z",
     "shell.execute_reply": "2022-07-12T03:56:01.664415Z",
     "shell.execute_reply.started": "2022-07-07T04:19:34.9049Z"
    },
    "papermill": {
     "duration": 0.332676,
     "end_time": "2022-07-12T03:56:01.664579",
     "exception": false,
     "start_time": "2022-07-12T03:56:01.331903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del IMAGES, MASKS\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5ac1f",
   "metadata": {
    "papermill": {
     "duration": 0.062735,
     "end_time": "2022-07-12T03:56:01.790288",
     "exception": false,
     "start_time": "2022-07-12T03:56:01.727553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📝 Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c56dbb0",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-07-12T03:56:01.925058Z",
     "iopub.status.busy": "2022-07-12T03:56:01.924402Z",
     "iopub.status.idle": "2022-07-12T03:56:02.668365Z",
     "shell.execute_reply": "2022-07-12T03:56:02.667139Z",
     "shell.execute_reply.started": "2022-07-07T04:19:39.896036Z"
    },
    "papermill": {
     "duration": 0.815462,
     "end_time": "2022-07-12T03:56:02.668512",
     "exception": false,
     "start_time": "2022-07-12T03:56:01.853050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "    \"id\":pred_ids,\n",
    "    \"class\":pred_classes,\n",
    "    \"predicted\":pred_strings\n",
    "})\n",
    "if not debug:\n",
    "    sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\n",
    "    del sub_df['predicted']\n",
    "else:\n",
    "    sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')\n",
    "    del sub_df['segmentation']\n",
    "    sub_df = sub_df[sub_df['id'].apply(lambda x: any([x.startswith(case_id) for case_id in cases]))]\n",
    "\n",
    "assert len(sub_df) == len(pred_df)\n",
    "assert len(test_dataset.df) * 3 == len(pred_df)\n",
    "sub_df = sub_df.merge(pred_df, on=['id','class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d900941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T03:56:02.807948Z",
     "iopub.status.busy": "2022-07-12T03:56:02.806911Z",
     "iopub.status.idle": "2022-07-12T03:56:02.832415Z",
     "shell.execute_reply": "2022-07-12T03:56:02.832956Z"
    },
    "papermill": {
     "duration": 0.101975,
     "end_time": "2022-07-12T03:56:02.833128",
     "exception": false,
     "start_time": "2022-07-12T03:56:02.731153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id        class predicted\n",
      "0  case123_day20_slice_0001  large_bowel          \n",
      "1  case123_day20_slice_0001  small_bowel          \n",
      "2  case123_day20_slice_0001      stomach          \n",
      "3  case123_day20_slice_0002  large_bowel          \n",
      "4  case123_day20_slice_0002  small_bowel          \n",
      "5  case123_day20_slice_0002      stomach          \n",
      "6  case123_day20_slice_0003  large_bowel          \n",
      "7  case123_day20_slice_0003  small_bowel          \n",
      "8  case123_day20_slice_0003      stomach          \n",
      "9  case123_day20_slice_0004  large_bowel          \n"
     ]
    }
   ],
   "source": [
    "sub_df.to_csv('submission.csv',index=False)\n",
    "print(sub_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1035.942361,
   "end_time": "2022-07-12T03:56:07.098943",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-12T03:38:51.156582",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
